<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="mikropml">
<title>Hyperparameter tuning • mikropml</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Hyperparameter tuning">
<meta property="og:description" content="mikropml">
<meta property="og:image" content="http://www.schlosslab.org/mikropml/logo.png">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">mikropml</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.6.0.9001</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Paper</h6>
    <a class="dropdown-item" href="../articles/paper.html">mikropml: User-Friendly R Package for Supervised Machine Learning Pipelines</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Vignettes</h6>
    <a class="dropdown-item" href="../articles/introduction.html">Introduction to mikropml</a>
    <a class="dropdown-item" href="../articles/preprocess.html">Preprocessing data</a>
    <a class="dropdown-item" href="../articles/tuning.html">Hyperparameter tuning</a>
    <a class="dropdown-item" href="../articles/parallel.html">Parallel processing</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/SchlossLab/mikropml/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Hyperparameter tuning</h1>
                        <h4 data-toc-skip class="author">Begüm D.
Topçuoğlu</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/SchlossLab/mikropml/blob/HEAD/vignettes/tuning.Rmd" class="external-link"><code>vignettes/tuning.Rmd</code></a></small>
      <div class="d-none name"><code>tuning.Rmd</code></div>
    </div>

    
    
<p>One particularly important aspect of machine learning (ML) is
hyperparameter tuning. A hyperparameter is a parameter that is set
before the ML training begins. These parameters are tunable and they
effect how well the model trains. We must do a grid search for many
hyperparameter possibilities and exhaust our search to pick the ideal
value for the model and dataset. In this package, we do this during the
cross-validation step.</p>
<p>Let’s start with an example ML run. The input data to
<code><a href="../reference/run_ml.html">run_ml()</a></code> is a dataframe where each row is a sample or
observation. One column (assumed to be the first) is the outcome of
interest, and all of the other columns are the features. We package
<code>otu_mini_bin</code> as a small example dataset with
<code>mikropml</code>.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("devtools")</span></span>
<span><span class="co"># devtools::install_github("SchlossLab/mikropml")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://www.schlosslab.org/mikropml/" class="external-link">mikropml</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">otu_mini_bin</span><span class="op">)</span></span>
<span><span class="co">#&gt;       dx Otu00001 Otu00002 Otu00003 Otu00004 Otu00005 Otu00006 Otu00007</span></span>
<span><span class="co">#&gt; 1 normal      350      268      213        1      208      230       70</span></span>
<span><span class="co">#&gt; 2 normal      568     1320       13      293      671      103       48</span></span>
<span><span class="co">#&gt; 3 normal      151      756      802      556      145      271       57</span></span>
<span><span class="co">#&gt; 4 normal      299       30     1018        0       25       99       75</span></span>
<span><span class="co">#&gt; 5 normal     1409      174        0        3        2     1136      296</span></span>
<span><span class="co">#&gt; 6 normal      167      712      213        4      332      534      139</span></span>
<span><span class="co">#&gt;   Otu00008 Otu00009 Otu00010</span></span>
<span><span class="co">#&gt; 1      230      235       64</span></span>
<span><span class="co">#&gt; 2      204      119      115</span></span>
<span><span class="co">#&gt; 3      176       37      710</span></span>
<span><span class="co">#&gt; 4       78      255      197</span></span>
<span><span class="co">#&gt; 5        1      537      533</span></span>
<span><span class="co">#&gt; 6      251      155      122</span></span></code></pre></div>
<p>Before we train and evaluate a ML model, we can preprocess the data.
You can learn more about this in the preprocessing vignette:
<code><a href="../articles/preprocess.html">vignette("preprocess")</a></code>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">preproc</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/preprocess_data.html">preprocess_data</a></span><span class="op">(</span></span>
<span>  dataset <span class="op">=</span> <span class="va">otu_mini_bin</span>,</span>
<span>  outcome_colname <span class="op">=</span> <span class="st">"dx"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Using 'dx' as the outcome column.</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="va">preproc</span><span class="op">$</span><span class="va">dat_transformed</span></span></code></pre></div>
<p>We’ll use <code>dat</code> for the following examples.</p>
<div class="section level2">
<h2 id="the-simplest-way-to-run_ml">The simplest way to <code>run_ml()</code><a class="anchor" aria-label="anchor" href="#the-simplest-way-to-run_ml"></a>
</h2>
<p>As mentioned above, the minimal input is your dataset
(<code>dataset</code>) and the machine learning model you want to use
(<code>method</code>).</p>
<p>When we <code><a href="../reference/run_ml.html">run_ml()</a></code>, by default we do a 100 times repeated,
5-fold cross-validation, where we evaluate the hyperparameters in these
500 total iterations.</p>
<p>Say we want to run L2 regularized logistic regression. We do this
with:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/run_ml.html">run_ml</a></span><span class="op">(</span><span class="va">dat</span>,</span>
<span>  <span class="st">"glmnet"</span>,</span>
<span>  outcome_colname <span class="op">=</span> <span class="st">"dx"</span>,</span>
<span>  cv_times <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">2019</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Using 'dx' as the outcome column.</span></span>
<span><span class="co">#&gt; Training the model...</span></span>
<span><span class="co">#&gt; Loading required package: ggplot2</span></span>
<span><span class="co">#&gt; Loading required package: lattice</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'caret'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:mikropml':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     compare_models</span></span>
<span><span class="co">#&gt; Training complete.</span></span></code></pre></div>
<p>You’ll probably get a warning when you run this because the dataset
is very small. If you want to learn more about that, check out the
introductory vignette about training and evaluating a ML model:
<code><a href="../articles/introduction.html">vignette("introduction")</a></code>.</p>
<p>By default, <code><a href="../reference/run_ml.html">run_ml()</a></code> selects hyperparameters depending
on the dataset and method used.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span><span class="op">$</span><span class="va">trained_model</span></span>
<span><span class="co">#&gt; glmnet </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 161 samples</span></span>
<span><span class="co">#&gt;  10 predictor</span></span>
<span><span class="co">#&gt;   2 classes: 'cancer', 'normal' </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; No pre-processing</span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (5 fold, repeated 100 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 128, 129, 129, 129, 129, 130, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   lambda  logLoss    AUC        prAUC      Accuracy   Kappa       F1       </span></span>
<span><span class="co">#&gt;   1e-04   0.7113272  0.6123301  0.5725828  0.5853927  0.17080523  0.5730989</span></span>
<span><span class="co">#&gt;   1e-03   0.7113272  0.6123301  0.5725828  0.5853927  0.17080523  0.5730989</span></span>
<span><span class="co">#&gt;   1e-02   0.7112738  0.6123883  0.5726478  0.5854514  0.17092470  0.5731635</span></span>
<span><span class="co">#&gt;   1e-01   0.6819806  0.6210744  0.5793961  0.5918756  0.18369829  0.5779616</span></span>
<span><span class="co">#&gt;   1e+00   0.6803749  0.6278273  0.5827655  0.5896356  0.17756961  0.5408139</span></span>
<span><span class="co">#&gt;   1e+01   0.6909820  0.6271894  0.5814202  0.5218000  0.02920942  0.1875293</span></span>
<span><span class="co">#&gt;   Sensitivity  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision</span></span>
<span><span class="co">#&gt;   0.5789667    0.5920074    0.5796685       0.5977166       0.5796685</span></span>
<span><span class="co">#&gt;   0.5789667    0.5920074    0.5796685       0.5977166       0.5796685</span></span>
<span><span class="co">#&gt;   0.5789667    0.5921250    0.5797769       0.5977182       0.5797769</span></span>
<span><span class="co">#&gt;   0.5805917    0.6032353    0.5880165       0.6026963       0.5880165</span></span>
<span><span class="co">#&gt;   0.5057833    0.6715588    0.6005149       0.5887829       0.6005149</span></span>
<span><span class="co">#&gt;   0.0607250    0.9678676    0.7265246       0.5171323       0.7265246</span></span>
<span><span class="co">#&gt;   Recall     Detection_Rate  Balanced_Accuracy</span></span>
<span><span class="co">#&gt;   0.5789667  0.2839655       0.5854870        </span></span>
<span><span class="co">#&gt;   0.5789667  0.2839655       0.5854870        </span></span>
<span><span class="co">#&gt;   0.5789667  0.2839636       0.5855458        </span></span>
<span><span class="co">#&gt;   0.5805917  0.2847195       0.5919135        </span></span>
<span><span class="co">#&gt;   0.5057833  0.2478291       0.5886711        </span></span>
<span><span class="co">#&gt;   0.0607250  0.0292613       0.5142963        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Tuning parameter 'alpha' was held constant at a value of 0</span></span>
<span><span class="co">#&gt; AUC was used to select the optimal model using the largest value.</span></span>
<span><span class="co">#&gt; The final values used for the model were alpha = 0 and lambda = 1.</span></span></code></pre></div>
<p>As you can see, the <code>alpha</code> hyperparameter is set to 0,
which specifies L2 regularization. <code>glmnet</code> gives us the
option to run both L1 and L2 regularization. If we change
<code>alpha</code> to 1, we would run L1-regularized logistic
regression. You can also tune <code>alpha</code> by specifying a variety
of values between 0 and 1. When you use a value that is between 0 and 1,
you are running elastic net. The default hyperparameter
<code>lambda</code> which adjusts the L2 regularization penalty is a
range of values between 10^-4 to 10.</p>
<p>When we look at the 100 repeated cross-validation performance metrics
such as <code>AUC</code>, <code>Accuracy</code>, <code>prAUC</code> for
each tested <code>lambda</code> value, we see that some are not
appropriate for this dataset and some do better than others.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span><span class="op">$</span><span class="va">trained_model</span><span class="op">$</span><span class="va">results</span></span>
<span><span class="co">#&gt;   alpha lambda   logLoss       AUC     prAUC  Accuracy      Kappa        F1</span></span>
<span><span class="co">#&gt; 1     0  1e-04 0.7113272 0.6123301 0.5725828 0.5853927 0.17080523 0.5730989</span></span>
<span><span class="co">#&gt; 2     0  1e-03 0.7113272 0.6123301 0.5725828 0.5853927 0.17080523 0.5730989</span></span>
<span><span class="co">#&gt; 3     0  1e-02 0.7112738 0.6123883 0.5726478 0.5854514 0.17092470 0.5731635</span></span>
<span><span class="co">#&gt; 4     0  1e-01 0.6819806 0.6210744 0.5793961 0.5918756 0.18369829 0.5779616</span></span>
<span><span class="co">#&gt; 5     0  1e+00 0.6803749 0.6278273 0.5827655 0.5896356 0.17756961 0.5408139</span></span>
<span><span class="co">#&gt; 6     0  1e+01 0.6909820 0.6271894 0.5814202 0.5218000 0.02920942 0.1875293</span></span>
<span><span class="co">#&gt;   Sensitivity Specificity Pos_Pred_Value Neg_Pred_Value Precision    Recall</span></span>
<span><span class="co">#&gt; 1   0.5789667   0.5920074      0.5796685      0.5977166 0.5796685 0.5789667</span></span>
<span><span class="co">#&gt; 2   0.5789667   0.5920074      0.5796685      0.5977166 0.5796685 0.5789667</span></span>
<span><span class="co">#&gt; 3   0.5789667   0.5921250      0.5797769      0.5977182 0.5797769 0.5789667</span></span>
<span><span class="co">#&gt; 4   0.5805917   0.6032353      0.5880165      0.6026963 0.5880165 0.5805917</span></span>
<span><span class="co">#&gt; 5   0.5057833   0.6715588      0.6005149      0.5887829 0.6005149 0.5057833</span></span>
<span><span class="co">#&gt; 6   0.0607250   0.9678676      0.7265246      0.5171323 0.7265246 0.0607250</span></span>
<span><span class="co">#&gt;   Detection_Rate Balanced_Accuracy   logLossSD      AUCSD    prAUCSD AccuracySD</span></span>
<span><span class="co">#&gt; 1      0.2839655         0.5854870 0.085315967 0.09115229 0.07296554 0.07628572</span></span>
<span><span class="co">#&gt; 2      0.2839655         0.5854870 0.085315967 0.09115229 0.07296554 0.07628572</span></span>
<span><span class="co">#&gt; 3      0.2839636         0.5855458 0.085276565 0.09122242 0.07301412 0.07637123</span></span>
<span><span class="co">#&gt; 4      0.2847195         0.5919135 0.048120032 0.09025695 0.07329214 0.07747312</span></span>
<span><span class="co">#&gt; 5      0.2478291         0.5886711 0.012189172 0.09111917 0.07505095 0.07771171</span></span>
<span><span class="co">#&gt; 6      0.0292613         0.5142963 0.001610008 0.09266875 0.07640896 0.03421597</span></span>
<span><span class="co">#&gt;      KappaSD       F1SD SensitivitySD SpecificitySD Pos_Pred_ValueSD</span></span>
<span><span class="co">#&gt; 1 0.15265728 0.09353786    0.13091452    0.11988406       0.08316345</span></span>
<span><span class="co">#&gt; 2 0.15265728 0.09353786    0.13091452    0.11988406       0.08316345</span></span>
<span><span class="co">#&gt; 3 0.15281903 0.09350099    0.13073501    0.12002481       0.08329024</span></span>
<span><span class="co">#&gt; 4 0.15485134 0.09308733    0.12870031    0.12037225       0.08554483</span></span>
<span><span class="co">#&gt; 5 0.15563046 0.10525917    0.13381009    0.11639614       0.09957685</span></span>
<span><span class="co">#&gt; 6 0.06527242 0.09664720    0.08010494    0.06371495       0.31899811</span></span>
<span><span class="co">#&gt;   Neg_Pred_ValueSD PrecisionSD   RecallSD Detection_RateSD Balanced_AccuracySD</span></span>
<span><span class="co">#&gt; 1       0.08384956  0.08316345 0.13091452       0.06394409          0.07640308</span></span>
<span><span class="co">#&gt; 2       0.08384956  0.08316345 0.13091452       0.06394409          0.07640308</span></span>
<span><span class="co">#&gt; 3       0.08385838  0.08329024 0.13073501       0.06384692          0.07648207</span></span>
<span><span class="co">#&gt; 4       0.08427362  0.08554483 0.12870031       0.06272897          0.07748791</span></span>
<span><span class="co">#&gt; 5       0.07597766  0.09957685 0.13381009       0.06453637          0.07773039</span></span>
<span><span class="co">#&gt; 6       0.02292294  0.31899811 0.08010494       0.03803159          0.03184136</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="customizing-hyperparameters">Customizing hyperparameters<a class="anchor" aria-label="anchor" href="#customizing-hyperparameters"></a>
</h2>
<p>In this example, we want to change the <code>lambda</code> values to
provide a better range to test in the cross-validation step. We don’t
want to use the defaults but provide our own named list with new
values.</p>
<p>For example:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_hp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  alpha <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  lambda <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.00001</span>, <span class="fl">0.0001</span>, <span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.015</span>, <span class="fl">0.02</span>, <span class="fl">0.025</span>, <span class="fl">0.03</span>, <span class="fl">0.04</span>, <span class="fl">0.05</span>, <span class="fl">0.06</span>, <span class="fl">0.1</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">new_hp</span></span>
<span><span class="co">#&gt; $alpha</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $lambda</span></span>
<span><span class="co">#&gt;  [1] 0.00001 0.00010 0.00100 0.01000 0.01500 0.02000 0.02500 0.03000 0.04000</span></span>
<span><span class="co">#&gt; [10] 0.05000 0.06000 0.10000</span></span></code></pre></div>
<p>Now let’s run L2 logistic regression with the new <code>lambda</code>
values:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/run_ml.html">run_ml</a></span><span class="op">(</span><span class="va">dat</span>,</span>
<span>  <span class="st">"glmnet"</span>,</span>
<span>  outcome_colname <span class="op">=</span> <span class="st">"dx"</span>,</span>
<span>  cv_times <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  hyperparameters <span class="op">=</span> <span class="va">new_hp</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">2019</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Using 'dx' as the outcome column.</span></span>
<span><span class="co">#&gt; Training the model...</span></span>
<span><span class="co">#&gt; Training complete.</span></span>
<span><span class="va">results</span><span class="op">$</span><span class="va">trained_model</span></span>
<span><span class="co">#&gt; glmnet </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 161 samples</span></span>
<span><span class="co">#&gt;  10 predictor</span></span>
<span><span class="co">#&gt;   2 classes: 'cancer', 'normal' </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; No pre-processing</span></span>
<span><span class="co">#&gt; Resampling: Cross-Validated (5 fold, repeated 100 times) </span></span>
<span><span class="co">#&gt; Summary of sample sizes: 128, 129, 129, 129, 129, 130, ... </span></span>
<span><span class="co">#&gt; Resampling results across tuning parameters:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   lambda   logLoss    AUC        prAUC      Accuracy   Kappa      F1       </span></span>
<span><span class="co">#&gt;   0.00001  0.7215038  0.6112253  0.5720005  0.5842184  0.1684871  0.5726974</span></span>
<span><span class="co">#&gt;   0.00010  0.7215038  0.6112253  0.5720005  0.5842184  0.1684871  0.5726974</span></span>
<span><span class="co">#&gt;   0.00100  0.7209099  0.6112771  0.5719601  0.5845329  0.1691285  0.5730414</span></span>
<span><span class="co">#&gt;   0.01000  0.6984432  0.6156112  0.5758977  0.5830960  0.1665062  0.5759265</span></span>
<span><span class="co">#&gt;   0.01500  0.6913332  0.6169396  0.5770496  0.5839720  0.1683912  0.5786347</span></span>
<span><span class="co">#&gt;   0.02000  0.6870103  0.6177313  0.5779563  0.5833645  0.1673234  0.5796891</span></span>
<span><span class="co">#&gt;   0.02500  0.6846387  0.6169757  0.5769305  0.5831907  0.1669901  0.5792840</span></span>
<span><span class="co">#&gt;   0.03000  0.6834369  0.6154763  0.5754118  0.5821394  0.1649081  0.5786336</span></span>
<span><span class="co">#&gt;   0.04000  0.6833322  0.6124776  0.5724802  0.5786224  0.1578750  0.5735757</span></span>
<span><span class="co">#&gt;   0.05000  0.6850454  0.6069059  0.5668928  0.5732197  0.1468699  0.5624480</span></span>
<span><span class="co">#&gt;   0.06000  0.6880861  0.5974311  0.5596714  0.5620224  0.1240112  0.5375824</span></span>
<span><span class="co">#&gt;   0.10000  0.6944846  0.5123565  0.3034983  0.5120114  0.0110144  0.3852423</span></span>
<span><span class="co">#&gt;   Sensitivity  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision</span></span>
<span><span class="co">#&gt;   0.5798500    0.5888162    0.5780748       0.5971698       0.5780748</span></span>
<span><span class="co">#&gt;   0.5798500    0.5888162    0.5780748       0.5971698       0.5780748</span></span>
<span><span class="co">#&gt;   0.5801167    0.5891912    0.5784544       0.5974307       0.5784544</span></span>
<span><span class="co">#&gt;   0.5883667    0.5783456    0.5755460       0.5977390       0.5755460</span></span>
<span><span class="co">#&gt;   0.5929750    0.5756471    0.5763123       0.5987220       0.5763123</span></span>
<span><span class="co">#&gt;   0.5967167    0.5708824    0.5748385       0.5990649       0.5748385</span></span>
<span><span class="co">#&gt;   0.5970250    0.5702721    0.5743474       0.5997928       0.5743474</span></span>
<span><span class="co">#&gt;   0.5964500    0.5687721    0.5734044       0.5982451       0.5734044</span></span>
<span><span class="co">#&gt;   0.5904500    0.5677353    0.5699817       0.5943308       0.5699817</span></span>
<span><span class="co">#&gt;   0.5734833    0.5736176    0.5668523       0.5864448       0.5668523</span></span>
<span><span class="co">#&gt;   0.5360333    0.5881250    0.5595918       0.5722851       0.5595918</span></span>
<span><span class="co">#&gt;   0.1145917    0.8963456    0.5255752       0.5132665       0.5255752</span></span>
<span><span class="co">#&gt;   Recall     Detection_Rate  Balanced_Accuracy</span></span>
<span><span class="co">#&gt;   0.5798500  0.28441068      0.5843331        </span></span>
<span><span class="co">#&gt;   0.5798500  0.28441068      0.5843331        </span></span>
<span><span class="co">#&gt;   0.5801167  0.28453770      0.5846539        </span></span>
<span><span class="co">#&gt;   0.5883667  0.28860521      0.5833561        </span></span>
<span><span class="co">#&gt;   0.5929750  0.29084305      0.5843110        </span></span>
<span><span class="co">#&gt;   0.5967167  0.29264681      0.5837995        </span></span>
<span><span class="co">#&gt;   0.5970250  0.29278708      0.5836485        </span></span>
<span><span class="co">#&gt;   0.5964500  0.29248583      0.5826110        </span></span>
<span><span class="co">#&gt;   0.5904500  0.28951992      0.5790926        </span></span>
<span><span class="co">#&gt;   0.5734833  0.28119862      0.5735505        </span></span>
<span><span class="co">#&gt;   0.5360333  0.26270204      0.5620792        </span></span>
<span><span class="co">#&gt;   0.1145917  0.05585777      0.5054686        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Tuning parameter 'alpha' was held constant at a value of 1</span></span>
<span><span class="co">#&gt; AUC was used to select the optimal model using the largest value.</span></span>
<span><span class="co">#&gt; The final values used for the model were alpha = 1 and lambda = 0.02.</span></span></code></pre></div>
<p>This time, we cover a larger and different range of
<code>lambda</code> settings in cross-validation.</p>
<p>How do we know which <code>lambda</code> value is the best one? To
answer that, we need to run the ML pipeline on multiple data splits and
look at the mean cross-validation performance of each
<code>lambda</code> across those modeling experiments. We describe how
to run the pipeline with multiple data splits in
<code><a href="../articles/parallel.html">vignette("parallel")</a></code>.</p>
<p>Here we train the model with the new <code>lambda</code> range we
defined above. We run it 3 times each with a different seed, which will
result in different splits of the data into training and testing sets.
We can then use <code>plot_hp_performance</code> to see which
<code>lambda</code> gives us the largest mean AUC value across modeling
experiments.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">102</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">seed</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="../reference/run_ml.html">run_ml</a></span><span class="op">(</span><span class="va">dat</span>, <span class="st">"glmnet"</span>, seed <span class="op">=</span> <span class="va">seed</span>, hyperparameters <span class="op">=</span> <span class="va">new_hp</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span><span class="co">#&gt; Using 'dx' as the outcome column.</span></span>
<span><span class="co">#&gt; Training the model...</span></span>
<span><span class="co">#&gt; Training complete.</span></span>
<span><span class="co">#&gt; Using 'dx' as the outcome column.</span></span>
<span><span class="co">#&gt; Training the model...</span></span>
<span><span class="co">#&gt; Training complete.</span></span>
<span><span class="co">#&gt; Using 'dx' as the outcome column.</span></span>
<span><span class="co">#&gt; Training the model...</span></span>
<span><span class="co">#&gt; Training complete.</span></span>
<span><span class="va">models</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="va">results</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">$</span><span class="va">trained_model</span><span class="op">)</span></span>
<span><span class="va">hp_metrics</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/combine_hp_performance.html">combine_hp_performance</a></span><span class="op">(</span><span class="va">models</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot_hp_performance.html">plot_hp_performance</a></span><span class="op">(</span><span class="va">hp_metrics</span><span class="op">$</span><span class="va">dat</span>, <span class="va">lambda</span>, <span class="va">AUC</span><span class="op">)</span></span></code></pre></div>
<p><img src="tuning_files/figure-html/unnamed-chunk-9-1.png" width="700"></p>
<p>As you can see, we get a mean maxima at <code>0.03</code> which is
the best <code>lambda</code> value for this dataset when we run 3 data
splits. The fact that we are seeing this maxima in the middle of our
range and not at the edges, shows that we are providing a large enough
range to exhaust our <code>lambda</code> search as we build the model.
We recommend the user to use this plot to make sure the best
hyperparameter is not on the edges of the provided list. For a better
understanding of the global maxima, it would be better to run more data
splits by using more seeds. We picked 3 seeds to keep the runtime down
for this vignette, but for real-world data we recommend using many more
seeds.</p>
</div>
<div class="section level2">
<h2 id="hyperparameter-options">Hyperparameter options<a class="anchor" aria-label="anchor" href="#hyperparameter-options"></a>
</h2>
<p>You can see which default hyperparameters would be used for your
dataset with <code><a href="../reference/get_hyperparams_list.html">get_hyperparams_list()</a></code>. Here are a few
examples with built-in datasets we provide:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/get_hyperparams_list.html">get_hyperparams_list</a></span><span class="op">(</span><span class="va">otu_mini_bin</span>, <span class="st">"glmnet"</span><span class="op">)</span></span>
<span><span class="co">#&gt; $lambda</span></span>
<span><span class="co">#&gt; [1] 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $alpha</span></span>
<span><span class="co">#&gt; [1] 0</span></span>
<span><span class="fu"><a href="../reference/get_hyperparams_list.html">get_hyperparams_list</a></span><span class="op">(</span><span class="va">otu_mini_bin</span>, <span class="st">"rf"</span><span class="op">)</span></span>
<span><span class="co">#&gt; $mtry</span></span>
<span><span class="co">#&gt; [1] 2 3 6</span></span>
<span><span class="fu"><a href="../reference/get_hyperparams_list.html">get_hyperparams_list</a></span><span class="op">(</span><span class="va">otu_small</span>, <span class="st">"rf"</span><span class="op">)</span></span>
<span><span class="co">#&gt; $mtry</span></span>
<span><span class="co">#&gt; [1]  4  8 16</span></span></code></pre></div>
<p>Here are the hyperparameters that are tuned for each of the modeling
methods. The output for all of them is very similar, so we won’t go into
those details.</p>
<div class="section level3">
<h3 id="regression">Regression<a class="anchor" aria-label="anchor" href="#regression"></a>
</h3>
<p>As mentioned above, <code>glmnet</code> uses the <code>alpha</code>
parameter and <code>lambda</code> hyperparameter. <code>alpha</code> of
<code>0</code> is for L2 regularization (ridge). <code>alpha</code> of
<code>1</code> is for L1 regularization (lasso). <code>alpha</code> in
between is elastic net. You can also tune <code>alpha</code> like you
would any other hyperparameter.</p>
<p>Please refer to original <code>glmnet</code> documentation for more
information: <a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html" class="external-link uri">https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></p>
<p>The default hyperparameters chosen by <code><a href="../reference/run_ml.html">run_ml()</a></code> are fixed
for <code>glmnet</code>.</p>
<pre><code><span><span class="co">#&gt; $lambda</span></span>
<span><span class="co">#&gt; [1] 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $alpha</span></span>
<span><span class="co">#&gt; [1] 0</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="random-forest">Random forest<a class="anchor" aria-label="anchor" href="#random-forest"></a>
</h3>
<p>When we run <code>rf</code> or <code>parRF</code>, we are using the
the <code>randomForest</code> package implementation. We are tuning the
<code>mtry</code> hyperparameter. This is the number of features that
are randomly collected to be sampled at each tree node. This number
needs to be less than the number of features in the dataset. Please
refer to the original documentation for more information: <a href="https://cran.r-project.org/web/packages/randomForest/randomForest.pdf" class="external-link uri">https://cran.r-project.org/web/packages/randomForest/randomForest.pdf</a></p>
<p>By default, we take the square root of number of features in the
dataset and we provide a range that is
<code>[sqrt_features / 2, sqrt_features, sqrt_features * 2]</code>.</p>
<p>For example if the number of features is 1000:</p>
<pre><code><span><span class="co">#&gt; $mtry</span></span>
<span><span class="co">#&gt; [1] 16 32 64</span></span></code></pre>
<p>Similar to the <code>glmnet</code> method, we can provide our own
<code>mtry</code> range.</p>
</div>
<div class="section level3">
<h3 id="decision-tree">Decision tree<a class="anchor" aria-label="anchor" href="#decision-tree"></a>
</h3>
<p>When we run <code>rpart2</code>, we are running the
<code>rpart</code> package implementation of decision tree. We are
tuning the <code>maxdepth</code> hyperparameter. This is the maximum
depth of any node of the final tree. Please refer to the original
documentation for more information on maxdepth: <a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf" class="external-link uri">https://cran.r-project.org/web/packages/rpart/rpart.pdf</a></p>
<p>By default, we provide a range that is less than the number of
features in the dataset.</p>
<p>For example if we have 1000 features:</p>
<pre><code><span><span class="co">#&gt; $maxdepth</span></span>
<span><span class="co">#&gt; [1]  1  2  4  8 16 30</span></span></code></pre>
<p>or 10 features:</p>
<pre><code><span><span class="co">#&gt; $maxdepth</span></span>
<span><span class="co">#&gt; [1] 1 2 4 8</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="svm-with-radial-basis-kernel">SVM with radial basis kernel<a class="anchor" aria-label="anchor" href="#svm-with-radial-basis-kernel"></a>
</h3>
<p>When we run the <code>svmRadial</code> method, we are tuning the
<code>C</code> and <code>sigma</code> hyperparameters.
<code>sigma</code> defines how far the influence of a single training
example reaches and <code>C</code> behaves as a regularization
parameter. Please refer to this great <code>sklearn</code> resource for
more information on these hyperparameters: <a href="https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html" class="external-link uri">https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html</a></p>
<p>By default, we provide 2 separate range of values for the two
hyperparameters.</p>
<pre><code><span><span class="co">#&gt; $C</span></span>
<span><span class="co">#&gt; [1] 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sigma</span></span>
<span><span class="co">#&gt; [1] 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="xgboost">XGBoost<a class="anchor" aria-label="anchor" href="#xgboost"></a>
</h3>
<p>When we run the <code>xgbTree</code> method, we are tuning the
<code>nrounds</code>, <code>gamma</code>, <code>eta</code>
<code>max_depth</code>, <code>colsample_bytree</code>,
<code>min_child_weight</code> and <code>subsample</code>
hyperparameters.</p>
<p>You can read more about these hyperparameters here: <a href="https://xgboost.readthedocs.io/en/latest/parameter.html" class="external-link uri">https://xgboost.readthedocs.io/en/latest/parameter.html</a></p>
<p>By default, we set the <code>nrounds</code>, <code>gamma</code>,
<code>colsample_bytree</code> and <code>min_child_weight</code> to fixed
values and we provide a range of values for <code>eta</code>,
<code>max_depth</code> and <code>subsample</code>. All of these can be
changed and optimized by the user by supplying a custom named list of
hyperparameters to <code><a href="../reference/run_ml.html">run_ml()</a></code>.</p>
<pre><code><span><span class="co">#&gt; $nrounds</span></span>
<span><span class="co">#&gt; [1] 100</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $gamma</span></span>
<span><span class="co">#&gt; [1] 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $eta</span></span>
<span><span class="co">#&gt; [1] 0.001 0.010 0.100 1.000</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $max_depth</span></span>
<span><span class="co">#&gt; [1]  1  2  4  8 16 30</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $colsample_bytree</span></span>
<span><span class="co">#&gt; [1] 0.8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $min_child_weight</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $subsample</span></span>
<span><span class="co">#&gt; [1] 0.4 0.5 0.6 0.7</span></span></code></pre>
</div>
</div>
<div class="section level2">
<h2 id="other-ml-methods">Other ML methods<a class="anchor" aria-label="anchor" href="#other-ml-methods"></a>
</h2>
<p>While the above ML methods are those that we have tested and set
default hyperparameters for, in theory you may be able use other methods
supported by caret with <code><a href="../reference/run_ml.html">run_ml()</a></code>. Take a look at the <a href="https://topepo.github.io/caret/available-models.html" class="external-link">available
models in caret</a> (or see <a href="https://topepo.github.io/caret/train-models-by-tag.html" class="external-link">here</a>
for a list by tag). You will need to give <code><a href="../reference/run_ml.html">run_ml()</a></code> your own
custom hyperparameters just like in the examples above:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/run_ml.html">run_ml</a></span><span class="op">(</span><span class="va">otu_mini_bin</span>,</span>
<span>  <span class="st">"regLogistic"</span>,</span>
<span>  hyperparameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    cost <span class="op">=</span> <span class="fl">10</span><span class="op">^</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4</span>, <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>    epsilon <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.01</span><span class="op">)</span>,</span>
<span>    loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"L2_primal"</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by <a href="https://github.com/BTopcuoglu" class="external-link">Begüm Topçuoğlu</a>, <a href="https://github.com/zenalapp" class="external-link">Zena Lapp</a>, <a href="https://github.com/kelly-sovacool" class="external-link">Kelly Sovacool</a>, Evan Snitkin, Jenna Wiens, <a href="https://github.com/pschloss" class="external-link">Patrick Schloss</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
